<details><summary>目录</summary><p>

- [Tokenizer 算法](#tokenizer-算法)
    - [BPE](#bpe)
    - [Unigram](#unigram)
- [Tokenzier 工具](#tokenzier-工具)
    - [tiktoken](#tiktoken)
    - [SentencePiecce](#sentencepiecce)
- [资料](#资料)
</p></details><p></p>

# Tokenizer 算法

## BPE

> Byte-Pair Encoding, BPE

* https://github.com/openai/gpt-2/blob/master/src/encoder.py
* https://github.com/rasbt/LLMs-from-scratch/blob/0911e71497769782975d68dba6e13f22157e5fb5/ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb
* https://github.com/rasbt/LLMs-from-scratch/blob/2dc46bedc6e86b79a16c4099e557564cd23e03ef/ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb
* https://github.com/rasbt/LLMs-from-scratch/blob/2dc46bedc6e86b79a16c4099e557564cd23e03ef/ch02/02_bonus_bytepair-encoder/bpe_openai_gpt2.py

## Unigram

# Tokenzier 工具

## tiktoken

## SentencePiecce

# 资料

* [如何实现一个分词器](https://juejin.cn/post/7397701403378155530)
